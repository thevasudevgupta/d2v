common:
  # these are shared for teacher & student
  vocab_size: null  # will be set inside script
  num_attention_heads: 8
  num_hidden_layers: 6
  hidden_size: 512
  intermediate_size: 2048

  # script arguments
  tokenizer_id: roberta-base
  is_fp16: false

student:
  # data2vec extra args
  num_head_layers: 2
  approximate_gelu: true

teacher:
  average_top_k_layers: 4

ema:
  ema_start_decay: 0.999
  ema_end_decay: 0.9999

data_collator:
  max_length: 512
  mlm_probability: 0.2

trainer:
  max_epochs: 10
  batch_size_per_device: 1
  wandb_project_name: data2vec-text
  epochs_save_dir: model_1
  logging_steps: 128
  max_steps_per_epoch: -1

optax:
  lr: 5.e-5
  init_lr: 1.e-7
  warmup_steps: 1500
  weight_decay: 1.e-3
